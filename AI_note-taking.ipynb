{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY=\"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Smart Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "def get_groq_mistral_response(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"mistral-saba-24b\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_completion_tokens=1024,\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "            stop=None,    \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Groq Error: {e}\")\n",
    "        return \"⚠️ Error from Groq/Mistral API.\"\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_highlight_prompt(text: str) -> str:\n",
    "    return f\"\"\"\n",
    "    You are an advanced document annotator with expertise in analyzing and classifying highlighted text. Your task is to provide a **precise and insightful annotation** for the given highlight.\n",
    "\n",
    "    1. **Classify the Highlight Type**:  \n",
    "       Choose from the following categories:\n",
    "       - **Concept** (A single idea, definition, or principle)  \n",
    "       - **Process** (Step-by-step explanation or structured reasoning)  \n",
    "       - **Fact** (A verified statement, research finding, or statistic)  \n",
    "       - **Task** (A to-do item or action-based instruction)  \n",
    "       - **Formula** (A structured or mathematical representation)  \n",
    "\n",
    "    2. **Classify the Sentence Type** (Choose the best match):  \n",
    "       - **Definition**  \n",
    "       - **Explanation**  \n",
    "       - **Quote**  \n",
    "       - **Important Fact**  \n",
    "       - **Recommendation**  \n",
    "       - **Author's Opinion**  \n",
    "       - **Analogy**  \n",
    "       - **Other (Specify if needed)**  \n",
    "\n",
    "    3. **Generate a Short Note**:  \n",
    "       - Clearly **explain the significance** of the highlight in 1-2 sentences.  \n",
    "       - Avoid generic phrases like \"This sentence describes...\"  \n",
    "       - Use **engaging and direct** phrasing: \"It highlights...\", \"It defines...\", \"It recommends...\".  \n",
    "\n",
    "    **Highlighted Sentence:**  \n",
    "    \\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "    **Respond ONLY in this format (no extra text):**  \n",
    "    - **Highlight Type**: [One of Concept, Process, Fact, Task, Formula]  \n",
    "    - **Sentence Type**: [One of Definition, Explanation, Quote, etc.]  \n",
    "    - **Short Note**: [A brief, professional annotation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Highlight Type**: Concept\n",
      "- **Sentence Type**: Definition\n",
      "- **Short Note**: It defines reinforcement learning as a method that uses reward feedback to train agents on performing tasks.\n"
     ]
    }
   ],
   "source": [
    "# Concept\n",
    "highlight1 = \"Reinforcement learning uses reward feedback to teach agents how to act.\"\n",
    "prompt = smart_highlight_prompt(highlight1)\n",
    "print(get_groq_mistral_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Highlight Type**: Concept\n",
      "- **Sentence Type**: Explanation\n",
      "- **Short Note**: It highlights the trade-offs and efficiency considerations between float16 and bfloat16 compared to float32 for representing numerical values, especially in the context of GPU performance during matrix multiplications.\n"
     ]
    }
   ],
   "source": [
    "# For paragraph\n",
    "highlight2 = \"Float16 can only represent numbers up to 65504, whilst bfloat16 can represent huge numbers up to 10^38! But notice both number formats use only 16bits! This is because float16 allocates more bits so it can represent smaller decimals better, whilst bfloat16 cannot represent fractions well. But why float16? Let's just use float32! But unfortunately float32 in GPUs is very slow for matrix multiplications - sometimes 4 to 10x slower! So we cannot do this.\"\n",
    "\n",
    "prompt = smart_highlight_prompt(highlight2)\n",
    "print(get_groq_mistral_response(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic summary prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_summary_prompt(highlights: list) -> str:\n",
    "  return f\"\"\"\n",
    "    You are an AI-powered summarization expert. Your task is to analyze the provided highlights and generate a **concise study memory** in a well-structured, high-impact paragraph.\n",
    "\n",
    "    ### **Instructions:**\n",
    "    1. **Capture the key insights** from the highlights without repeating information.  \n",
    "    2. **Group related ideas** for a logical flow (e.g., learning techniques, AI advancements, safety concerns).  \n",
    "    3. **Make it clear, precise, and engaging**, avoiding unnecessary filler words.  \n",
    "    4. **Ensure natural readability** – the summary should feel polished and professional.\n",
    "\n",
    "    ### **Highlights to Summarize:**  \n",
    "    {highlights}\n",
    "\n",
    "    ### **Respond in the following format:**  \n",
    "    - **Compressed Study Memory:**  \n",
    "      - [A single, structured paragraph summarizing the highlights effectively]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Compressed Study Memory:**\n",
      "  Machine learning encompasses various techniques including backpropagation, which trains neural networks through error feedback and weight adjustments, and reinforcement learning, where models learn by receiving rewards or penalties. Activation functions determine neuron activation in neural networks, while gradient descent optimizes model parameters. Deep learning, surpassing human capabilities in perceptual tasks, involves deep neural networks with multiple hidden layers. Ensuring model generalizability requires dropout regularization to prevent overfitting, and combining transfer learning with fine-tuning on domain-specific datasets enhances performance. Notable milestones include OpenAI's ChatGPT hitting 100 million users in two months, highlighting AI's rapid growth. Safety remains a pressing concern; Elon Musk emphasizes stringent regulation to mitigate potential catastrophic risks.\n"
     ]
    }
   ],
   "source": [
    "highlight1 = \"Backpropagation is a supervised learning algorithm used to train neural networks by adjusting weights based on error feedback.\"\n",
    "highlight2 = \"The activation function in a neural network determines whether a neuron should be activated based on weighted inputs.\"\n",
    "highlight3 = \"Reinforcement learning allows an agent to learn by receiving rewards or penalties, optimizing future decisions based on past actions.\" \n",
    "highlight4 = \"Gradient descent is an optimization algorithm that adjusts model parameters by computing the gradient of the loss function.\"\n",
    "highlight5 = \"As Geoffrey Hinton once said, 'Deep learning is the first technology in history that can solve perceptual problems better than humans.'\"\n",
    "highlight6 = \"Elon Musk argues that AI safety is humanity’s biggest challenge, requiring careful regulation to prevent catastrophic outcomes.\" \n",
    "highlight7 = \"Neural networks with more than three hidden layers are commonly referred to as deep neural networks.\"\n",
    "highlight8 = \"In 2023, OpenAI's ChatGPT reached 100 million users within just two months, making it the fastest-growing app in history.\"\n",
    "highlight9 = \"To improve model generalization, always use dropout regularization to prevent overfitting in deep learning models.\"\n",
    "highlight10 = \"For high-performance AI models, use a combination of transfer learning and fine-tuning on domain-specific datasets.\"\n",
    "\n",
    "highlights = [highlight1, highlight2, highlight3, highlight4, highlight5, highlight6, highlight7, highlight8, highlight9, highlight10]\n",
    "prompt = semantic_summary_prompt(highlights)\n",
    "print(get_groq_mistral_response(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Referencing(Multi-doc magic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/miniforge3/envs/NCvenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "import uuid\n",
    "\n",
    "# ✅ Initialize embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Initialize ChromaDB with persistent storage\n",
    "chroma_client = chromadb.PersistentClient(path=\"data/chroma_db\")\n",
    "\n",
    "# ✅ Get or create the 'notes' collection\n",
    "collection = chroma_client.get_or_create_collection(name=\"notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Add new note function\n",
    "def add_note(text, metadata=None):\n",
    "    note_id = str(uuid.uuid4())  # Unique ID\n",
    "    embedding = embedding_model.encode(text)\n",
    "    collection.add(\n",
    "        documents=[text],\n",
    "        embeddings=[embedding],\n",
    "        ids=[note_id],\n",
    "        metadatas=[metadata or {}]\n",
    "    )\n",
    "    return note_id, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Similar notes search\n",
    "def find_similar_notes(query_embedding, current_note_id=None, top_k=3):\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k + 1  # Fetch extra to allow filtering\n",
    "    )\n",
    "    \n",
    "    # Filter out current note\n",
    "    filtered_notes = []\n",
    "    for note_id, doc in zip(results[\"ids\"][0], results[\"documents\"][0]):\n",
    "        if note_id != current_note_id:\n",
    "            filtered_notes.append(doc)\n",
    "        if len(filtered_notes) >= top_k:\n",
    "            break\n",
    "\n",
    "    return filtered_notes\n",
    "\n",
    "\n",
    "# ✅ Example usage\n",
    "new_note = \"GANs generate realistic images.\"\n",
    "note_id, embedding = add_note(new_note, metadata={\"type\": \"note\", \"topic\": \"AI\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_note = \"VAEs also generate images, but their latent space is continuous.\"\n",
    "note_id, embedding = add_note(new_note, metadata={\"type\": \"note\", \"topic\": \"AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_note(\"Transformers revolutionized NLP tasks.\", metadata={\"type\": \"note\", \"topic\": \"AI\"}) \n",
    "add_note(\"GANs are useful for data augmentation.\", metadata={\"type\": \"note\", \"topic\": \"AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Similar Notes:\n",
      "1. GANs generate realistic images.\n",
      "2. GANs are useful for data augmentation.\n",
      "3. Transformers revolutionized NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "similar = find_similar_notes(embedding, current_note_id=note_id)\n",
    "\n",
    "print(\"🔍 Similar Notes:\")\n",
    "for i, note in enumerate(similar):\n",
    "    print(f\"{i+1}. {note}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NCvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
